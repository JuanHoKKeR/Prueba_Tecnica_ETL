# Google Cloud Workflows - Orquestación del Pipeline ETL
# Este workflow orquesta todo el proceso ETL de forma más robusta que un simple cron

main:
  params: [args]
  steps:
    # Step 1: Inicializar variables
    - init:
        assign:
          - project_id: ${sys.get_env("GCP_PROJECT_ID")}
          - cloud_run_url: "https://roda-analytics.juancruzdev.net"
          - bigquery_dataset: "roda_analytics"
          - execution_id: ${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
          - start_time: ${sys.now()}
          - zones_to_process: ["CHAPINERO", "KENNEDY", "USAQUEN", "SUBA", "TEUSAQUILLO", "ENGATIVA", "FONTIBON", "BOSA", "CIUDAD BOLIVAR", "SANTA FE"]
          - errors: []

    # Step 2: Verificar salud del servicio
    - checkHealth:
        try:
          call: http.get
          args:
            url: ${cloud_run_url + "/health"}
            timeout: 10
          result: health_response
        except:
          as: e
          steps:
            - logHealthError:
                call: sys.log
                args:
                  text: ${"Health check failed: " + json.encode_to_string(e)}
                  severity: "ERROR"
            - raiseHealthError:
                raise: ${"Service unhealthy: " + json.encode_to_string(e)}

    # Step 3: Trigger procesamiento batch
    - triggerETL:
        try:
          call: http.post
          args:
            url: ${cloud_run_url + "/process"}
            headers:
              Content-Type: "application/json"
            body:
              mode: "BATCH"
              zones: ${zones_to_process}
              force_refresh: true
            timeout: 30
          result: etl_response
        except:
          as: e
          steps:
            - logETLError:
                call: sys.log
                args:
                  text: ${"ETL trigger failed: " + json.encode_to_string(e)}
                  severity: "ERROR"
            - raiseETLError:
                raise: ${"Failed to start ETL: " + json.encode_to_string(e)}

    - extractJobId:
        assign:
          - job_id: ${etl_response.body.job_id}
          - job_status: ${etl_response.body.status}

    # Step 4: Monitorear progreso del job
    - waitForCompletion:
        steps:
          - defineCheckLoop:
              for:
                value: iteration
                range: [1, 60]  # Máximo 60 intentos (10 minutos)
                steps:
                  - checkJobStatus:
                      call: http.get
                      args:
                        url: ${cloud_run_url + "/process/" + job_id}
                        timeout: 10
                      result: job_status_response
                  
                  - evaluateStatus:
                      switch:
                        - condition: ${job_status_response.body.status == "COMPLETED"}
                          next: jobCompleted
                        - condition: ${job_status_response.body.status == "FAILED"}
                          next: jobFailed
                  
                  - waitBeforeRetry:
                      call: sys.sleep
                      args:
                        seconds: 10

    - jobTimeout:
        call: sys.log
        args:
          text: "Job timeout after 10 minutes"
          severity: "WARNING"
        next: analyzeZones

    - jobCompleted:
        call: sys.log
        args:
          text: ${"ETL job completed successfully: " + job_id}
          severity: "INFO"
        next: analyzeZones

    - jobFailed:
        call: sys.log
        args:
          text: ${"ETL job failed: " + job_id}
          severity: "ERROR"
        next: notifyFailure

    # Step 5: Analizar cada zona y aplicar ML
    - analyzeZones:
        parallel:
          for:
            value: zone
            in: ${zones_to_process}
            steps:
              - analyzeZone:
                  try:
                    call: http.get
                    args:
                      url: ${cloud_run_url + "/analyze/" + zone + "?include_ml=true"}
                      timeout: 30
                    result: zone_analysis
                  except:
                    as: e
                    steps:
                      - logZoneError:
                          call: sys.log
                          args:
                            text: ${"Failed to analyze zone " + zone + ": " + json.encode_to_string(e)}
                            severity: "WARNING"
                      - addToErrors:
                          assign:
                            - errors: ${list.concat(errors, [zone])}

    # Step 6: Validar datos en BigQuery
    - validateBigQuery:
        try:
          call: googleapis.bigquery.v2.jobs.query
          args:
            projectId: ${project_id}
            body:
              query: ${"SELECT COUNT(*) as count, MAX(calculation_date) as latest_date FROM `" + project_id + "." + bigquery_dataset + ".zone_safety_scores` WHERE calculation_date = CURRENT_DATE()"}
              useLegacySql: false
          result: bq_validation
        except:
          as: e
          steps:
            - logBQError:
                call: sys.log
                args:
                  text: ${"BigQuery validation failed: " + json.encode_to_string(e)}
                  severity: "WARNING"

    # Step 7: Generar reporte de ejecución
    - generateReport:
        assign:
          - end_time: ${sys.now()}
          - duration_seconds: ${int(end_time - start_time)}
          - report:
              execution_id: ${execution_id}
              start_time: ${start_time}
              end_time: ${end_time}
              duration_seconds: ${duration_seconds}
              zones_processed: ${zones_to_process}
              errors: ${errors}
              status: ${if(len(errors) > 0, "COMPLETED_WITH_ERRORS", "SUCCESS")}

    # Step 8: Guardar métricas en BigQuery
    - saveMetrics:
        try:
          call: googleapis.bigquery.v2.jobs.query
          args:
            projectId: ${project_id}
            body:
              query: ${
                "INSERT INTO `" + project_id + "." + bigquery_dataset + ".workflow_executions` " +
                "(execution_id, start_time, end_time, duration_seconds, zones_processed, error_count, status) VALUES (" +
                "'" + execution_id + "', " +
                "TIMESTAMP('" + start_time + "'), " +
                "TIMESTAMP('" + end_time + "'), " +
                duration_seconds + ", " +
                len(zones_to_process) + ", " +
                len(errors) + ", " +
                "'" + report.status + "')"
              }
              useLegacySql: false
        except:
          as: e
          steps:
            - logMetricsError:
                call: sys.log
                args:
                  text: "Failed to save metrics"
                  severity: "WARNING"

    # Step 9: Notificar resultado (opcional - si tienes Pub/Sub o email configurado)
    - notifySuccess:
        call: sys.log
        args:
          text: ${json.encode_to_string(report)}
          severity: "INFO"
        next: end

    - notifyFailure:
        call: sys.log
        args:
          text: "ETL Pipeline failed"
          severity: "ERROR"
        next: end

    - end:
        return: ${report}